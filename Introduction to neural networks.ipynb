{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalizing linear regression- Perceptrons \n",
    "\n",
    "\n",
    "In part-1 of the session we developed a model for predicting house prices using linear regression. As the name suggests, the linear regression aims to find a linear fit that best describes the training data. However, not **all relationships are linear**.\n",
    "\n",
    "Note that for a *nonlinear data*, it does not mattter how much training data we get linear regression is not able to clearly capture the dynamics of the system, as demonstrated in the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "%pylab inline\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you bring a plastic spoon to a knife fight \n",
    "\n",
    "Suppose we have small dataset and want to construct a model based on the given data. \n",
    "\n",
    "Based on the known data (red points), a linear model (blue line) can be constructed and suppose it results in satisfactory results for the chosen evaluation metrics.\n",
    "\n",
    "<img src=\"./snippet.jpg\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By contructing a model, we aim to learn the function that describes the relation between the $x$ and $y$: $$y = f(x).$$\n",
    "\n",
    "\n",
    "Our **goal is to learn**- $f$. \n",
    "\n",
    "#### More data arrives for the show\n",
    "<img src=\"./data.jpg\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using linear regression to fit this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Getting the data from a function in another (python) file \n",
    "from util_model_evals import MoreData, linear_performance\n",
    "\n",
    "X,Y = MoreData(N=100)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "linear = LinearRegression()\n",
    "\n",
    "linear.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_train = linear.predict(X_train)\n",
    "Y_pred_test = linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the data\n",
    "fig2 = plt.figure(figsize=(5, 5))\n",
    "fig2.clf()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "ax2.scatter(X_train,Y_train,color='r',marker='o')\n",
    "ax2.plot(X_train,Y_pred_train,color='b')\n",
    "ax2.set_xlabel('x');\n",
    "ax2.set_ylabel('y');\n",
    "ax2.set_title('Training set');\n",
    "#fig2.savefig('./images/X_train.jpg')\n",
    "\n",
    "fig3= plt.figure(figsize=(5, 5))\n",
    "fig3.clf()\n",
    "ax3 = fig3.add_subplot(111)\n",
    "ax3.scatter(X_test,Y_test,color='r',marker='o')\n",
    "ax3.plot(X_test,Y_pred_test,color='b')\n",
    "ax3.set_xlabel('x');\n",
    "ax3.set_ylabel('y');\n",
    "ax3.set_title('Test set');\n",
    "#fig3.savefig('./images/X_test.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When in doubt train on a larger dataset\n",
    "\n",
    "In most machine learning application, often training the model on larger data, results in more accurate prediction, however, in this instance, the inability of linear model to *learn the nonlinearity* of the underlying data does not reap the benefit of larger tarining data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes = [160,320,640,1280,2560,5120,10240,20480,40960]\n",
    "\n",
    "mse_list = linear_performance(train_sizes,X_test,Y_test,linear)\n",
    "\n",
    "fig4= plt.figure()\n",
    "fig4.clf()\n",
    "ax4 = fig4.add_subplot(111)\n",
    "ax4.plot(train_sizes, mse_list)\n",
    "ax4.set_xlabel('Train set size');\n",
    "ax4.set_ylabel('MSE');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter how large the training set size, the linear model is not able to represent the dynamics of the given data *correctly*.\n",
    "\n",
    "But the data that we are trying fit is described by the chaotic function $$y =  \\sin \\Big(\\frac{1}{x}\\Big),$$ which any state of the art machine learning model might have problems learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the f_x as an inline function using the built-in function- lambda\n",
    "#f_x = lambda x:np.sin(np.pi*x)*np.exp(-2.*np.pi*x)   \n",
    "f_x = lambda x:np.sin(1./x)\n",
    "# plotting the function \n",
    "Xs = np.linspace(-0.1,0.1,501)\n",
    "Ys = f_x(Xs) \n",
    "\n",
    "fig1 = plt.figure()\n",
    "fig1.clf()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "ax1.plot(Xs,Ys)\n",
    "ax1.set_xlabel('x');\n",
    "ax1.set_ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons improving linear regression \n",
    "\n",
    "### History \n",
    "To enhance the capability of linear models in learning nonlinear distribution, *Frank Rosenblatt* working at the Cornell Aeronautical Laboratory in 1957 proposed a family of artificial neural networks (ANN) for pattern classification and information storage. The algorithm came to be known as **perceptron**. The perceptron in its formulation was argued to be a simplified model of biological *neuron.* \n",
    "\n",
    "Since the very beginning, perceptron have attracted a lot of controversy, with Rosenblatt himself reporting the perceptron to be *\"the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\"*\n",
    "\n",
    "Although very promising, early researcher were quick in discovering that the perceptron could not be trained to recognise many classes of patterns. In their book titled *Perceptron*, Minsky and Papert showed that single layer perceptron are only capapble of learning linearly separable patterns and that it is impossible to learn an XOR function.\n",
    "\n",
    "Very soon perceptron became history, until it was realized that *multilayer perceptron* or  *feedforward neural networks* had greater ability in learning patterns than a single layer perceptron. It was shown that a multilayer perceptron could very well learn a XOR function. Still the widely cited work of Minsky and Papert resulted in decline of interest and funcding in research on neural networks, with the text being  reprinted in 1987 as *\"Perceptrons - Expanded Edition\"* where some errors in the original text were demonstrated and corrected.\n",
    "\n",
    "### Formulation \n",
    "\n",
    "A single layer perceptron (SLP) can be visualized as:\n",
    "\n",
    "<img src=\"./perceptron.png\" />\n",
    "\n",
    "The perceptron maps an input $\\mathbf{x}$ ( $m$-dimensional vector, $\\mathbf{x} \\in \\mathbb{R}^m$) to an output $y_k$, with $$\\hat{y} = \\sum_{i=0}^m w_i x_i,$$ where $x_i$, $w_i$ denote the feature of the input $\\mathbf{X}$ and the ghts accociated with the neurons. Observe that the first neuron has an value $1$ and the woight $w_0$, which translates to $$\\hat{y}\\sum_{i=1}^{m}w_ix_i + b.$$\n",
    "where $b:=w_0$, is ccommonly known a bias. For a linear classification problem, adding bias corresds to shiting/tranlation of the decision boundary.\n",
    "\n",
    "The basic idea is of a the perceptron is simply to give each input a relative score using the corresponding weights, thus **allowing information that is more relevant to be more dominant in the prediction.**\n",
    "\n",
    "### So whats the big deal??\n",
    "#### Multi-layer perceptron\n",
    "In a **multi-layer perceptron** a number of hidden layers are included, it is observed that deeper(more hidden layers) and wider (more neurons) networks are quite remarkable in learning more complex representation\n",
    "    \n",
    "In its formulation multi layer perceptrons are able to learn the nonlinearities represented by the data. Using the nonlinear activations like $\\tanh$, *Relu (rectified linear units)*, *sigmoid* further enable to learn the underlying distribution more efficiently.\n",
    "\n",
    "#### How do I update the weights after each iteration of the optimzation (like gradient descent)?\n",
    "The most significant ability of neural network, is their ability to allow the back progation of the information (gradient of the loss function) from the output layer to the input layer, using *back propogation*, which in simple terms is **chain rule**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptrons for predicting Boston House prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "bos = pd.DataFrame(boston.data)\n",
    "bos.columns = boston.feature_names\n",
    "## import the perceptron model sklearn.linear_model\n",
    "bos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating the feature vector X and the label Y\n",
    "bos['PRICE'] = boston.target\n",
    "\n",
    "X = bos.drop('PRICE', axis = 1)\n",
    "Y = bos['PRICE']  # label that we want to predict after learning the function f, Y~ f(X)\n",
    "\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train and test sets\n",
    "test_train_ratio = 0.4\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = test_train_ratio, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of training sample = '+str(X_train.shape[0]))\n",
    "print('Number of test sample = '+str(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recalling linear regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_lin = linear.predict(X_test)\n",
    "print('Mean square error for linear regression ='+str(mse(Y_test,Y_pred_lin)))\n",
    "\n",
    "fig4= plt.figure(figsize=(5, 5))\n",
    "fig4.clf()\n",
    "ax4 = fig4.add_subplot(111)\n",
    "ax4.scatter(Y_test, Y_pred_lin,color='b',marker='o')\n",
    "ax4.plot(Y_test,Y_test,color='k')\n",
    "ax4.set_xlabel('Actual price');\n",
    "ax4.set_ylabel('Predicted price');\n",
    "ax4.set_title('Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise :\n",
    "Change the test train ratio and observe how the mse and fit changes.\n",
    "\n",
    "*Hint*: Update the variable test_train_ratio and re-run the above blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of fetures = '+str(X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron- hyperparameter jargon\n",
    "\n",
    "\n",
    "To demstrate the multi-layer perceptron we use a built-in function of MLPRegressor located in sklearn.neural_network, documentation: http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the hyperparametes_____________________________________________________________________\n",
    "activation = 'identity'\n",
    "structure = (23,5)  # (neurons, hidden layers)\n",
    "optimizer = 'adam'\n",
    "learn_rate_type = 'adaptive' # {‘constant’, ‘invscaling’, ‘adaptive’}\n",
    "learn_rate = 1.0E-05\n",
    "iter_max = 1000\n",
    "validation_ratio = 0.25\n",
    "\n",
    "##_____________________________________________________________________________________________  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(activation=activation, hidden_layer_sizes=structure,solver=optimizer,\n",
    "                   learning_rate=learn_rate_type,learning_rate_init=learn_rate,random_state=1234,\n",
    "                   validation_fraction=validation_ratio)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(X_train, Y_train)\n",
    "\n",
    "fig6= plt.figure(figsize=(5, 5))\n",
    "fig6.clf()\n",
    "ax6 = fig6.add_subplot(111)\n",
    "Y_pred_mlp = mlp.predict(X_test)\n",
    "ax6.plot(mlp.loss_curve_)\n",
    "ax6.set_xlabel('Evolution of the cost function');\n",
    "ax6.set_ylabel('Cost function');\n",
    "ax5.set_title('Number of steps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the neural network\n",
    "\n",
    "Our trained neural network can be visualized using the weights associated with each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriving the weights and biases\n",
    "hidden_layer = np.int64(1)\n",
    "weights = mlp.coefs_[hidden_layer]\n",
    "bias = mlp.intercepts_[hidden_layer]\n",
    "\n",
    "fig7= plt.figure(figsize=(5, 5))\n",
    "fig7.clf()\n",
    "ax7 = fig7.add_subplot(111)\n",
    "ax7.imshow(np.transpose(weights), cmap=plt.get_cmap(\"gray\"), aspect=\"auto\")\n",
    "ax7.set_xlabel('Neuron in '+ str(hidden_layer-1)+' layer');\n",
    "ax7.set_ylabel('Neuron in '+ str(hidden_layer+1)+' layer');\n",
    "ax7.set_title('Weights of hidden layer : '+str(hidden_layer));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to interpet such a visualization? \n",
    "First, on gray scale large negative numbers are black, large positive numbers are white, and numbers near zero are gray. Now we know that each neuron is taking it's weighted input and applying the logistic transformation on it, which outputs 0 for inputs much less than 0 and outputs 1 for inputs much greater than 0. So, for instance, if a particular weight $\\mathbf{w}^{(l)}_ij$ is large and negative it means that neuron $i$ is having its output strongly pushed to zero by the input from neuron $j$ of the underlying layer. If a pixel is gray then that means that neuron $i$ isn't very sensitive to the output of neuron $j$ in the layer below it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "print('')\n",
    "print('Mean square error for linear regression ='+str(mse(Y_test,Y_pred_mlp)))\n",
    "\n",
    "fig5= plt.figure(figsize=(5, 5))\n",
    "fig5.clf()\n",
    "ax5 = fig5.add_subplot(111)\n",
    "ax5.scatter(Y_test, Y_pred_mlp,color='b',marker='o')\n",
    "ax5.plot(Y_test,Y_test,color='k')\n",
    "ax5.set_xlabel('Actual price');\n",
    "ax5.set_ylabel('Predicted price');\n",
    "ax5.set_title('Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig5= plt.figure(figsize=(5, 5))\n",
    "fig5.clf()\n",
    "ax5 = fig5.add_subplot(111)\n",
    "ax5.scatter(Y_test, Y_pred_mlp,color='b',marker='o')\n",
    "ax5.plot(Y_test,Y_test,color='k')\n",
    "ax5.set_xlabel('Actual price');\n",
    "ax5.set_ylabel('Predicted price');\n",
    "ax5.set_title('Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does tuning mean\n",
    "\n",
    "I guess you might have figured it out- choosing the optimal set of hyperparameters to get the best (depending on the chosen metrics) model. \n",
    "\n",
    "\n",
    "### Competetion: Tune the hyperparameters to get the least mean squared value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading \n",
    "\n",
    "Multi layer perceptrons are the most basic form of feed-forward neural network. In the past five year complex architecture of neural networks have been applied to a variety of interest and have resulted in very promising results, leading to exponential burst of what is more commonly known as **deep learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
